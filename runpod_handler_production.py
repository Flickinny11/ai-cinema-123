#!/usr/bin/env python3
"""
RunPod Serverless Handler - Production Ready
Fixed AsyncIO and Volume Integration
August 2025 Edition
"""

import runpod
import logging
import json
import traceback
import os
import time
import subprocess
import boto3
from pathlib import Path
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor
import threading

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global pipeline instance - initialized once when worker starts
pipeline = None
pipeline_lock = threading.Lock()
executor = ThreadPoolExecutor(max_workers=2)

# Volume configuration
VOLUME_ID = "wv11ilzha0"
S3_ENDPOINT = "https://s3api-us-ks-2.runpod.io"
S3_REGION = "us-ks-2"

def setup_volume_access():
    """Setup access to the RunPod volume via S3"""
    try:
        logger.info("ðŸ”§ Setting up volume access...")
        
        # Create S3 client for RunPod volume
        s3_client = boto3.client(
            's3',
            endpoint_url=S3_ENDPOINT,
            region_name=S3_REGION,
            aws_access_key_id=os.getenv('RUNPOD_AI_API_KEY', ''),
            aws_secret_access_key=os.getenv('RUNPOD_AI_API_KEY', '')
        )
        
        # Test connection
        try:
            response = s3_client.list_objects_v2(Bucket=VOLUME_ID, MaxKeys=1)
            logger.info("âœ… Volume access configured successfully")
            return s3_client
        except Exception as e:
            logger.warning(f"âš ï¸ Volume access test failed: {e}")
            return None
            
    except Exception as e:
        logger.error(f"âŒ Volume setup failed: {e}")
        return None

def download_model_from_hf(model_name: str, repo_id: str, cache_dir: str):
    """Download model from Hugging Face to volume"""
    try:
        logger.info(f"ðŸ“¥ Downloading {model_name} from {repo_id}...")
        
        # Set environment variables for HF
        os.environ["HF_HOME"] = "/runpod-volume/cache"
        os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
        
        # Create cache directory
        Path(cache_dir).mkdir(parents=True, exist_ok=True)
        
        # Check if already downloaded
        marker_file = Path(cache_dir) / f".{model_name}_downloaded"
        if marker_file.exists():
            logger.info(f"âœ… {model_name} already downloaded")
            return True
        
        # Download using huggingface_hub
        from huggingface_hub import snapshot_download
        
        snapshot_download(
            repo_id=repo_id,
            cache_dir=cache_dir,
            resume_download=True,
            local_dir_use_symlinks=False,
            ignore_patterns=["*.md", "*.py", "*.git*", "*.gitignore"]
        )
        
        # Create marker file
        marker_file.touch()
        logger.info(f"âœ… {model_name} downloaded successfully")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to download {model_name}: {e}")
        return False

def initialize_pipeline():
    """Initialize pipeline on worker startup with robust error handling"""
    global pipeline
    
    with pipeline_lock:
        if pipeline is not None:
            return pipeline
            
        logger.info("="*60)
        logger.info("ðŸŽ¬ Cinema AI Production Pipeline v2.1")
        logger.info("="*60)
        
        try:
            # Setup volume access
            s3_client = setup_volume_access()
            
            # Check PyTorch first
            try:
                import torch
                logger.info(f"âœ… PyTorch: {torch.__version__}")
                logger.info(f"âœ… CUDA Available: {torch.cuda.is_available()}")
                
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    logger.info(f"âœ… GPU: {gpu_name}")
                    logger.info(f"âœ… VRAM: {vram:.1f}GB")
                    
                    # Determine mode based on VRAM
                    if vram >= 80:
                        logger.info("ðŸŽ¬ Full Cinema Mode: All models enabled")
                        mode = "cinema"
                    elif vram >= 40:
                        logger.info("âš¡ Balanced Mode: Optimized models")
                        mode = "balanced"
                    else:
                        logger.info("ðŸš€ Fast Mode: Consumer GPU optimized")
                        mode = "fast"
                else:
                    logger.warning("âš ï¸ No GPU detected")
                    mode = "cpu"
                    
            except ImportError:
                logger.error("âŒ PyTorch not available")
                return None

            # Set up model directories with volume mount
            model_dirs = {
                "cache": "/runpod-volume/cache",
                "ltx": "/runpod-volume/ltx",
                "hunyuan": "/runpod-volume/hunyuan", 
                "musicgen": "/runpod-volume/musicgen",
                "audiogen": "/runpod-volume/audiogen",
                "xtts": "/runpod-volume/xtts"
            }
            
            # Create directories
            for name, path in model_dirs.items():
                Path(path).mkdir(parents=True, exist_ok=True)
                logger.info(f"ðŸ“ Created directory: {path}")

            # Download essential models if not present
            essential_models = [
                ("ltx", "Lightricks/LTX-Video", model_dirs["ltx"]),
                ("hunyuan", "tencent/HunyuanVideo", model_dirs["hunyuan"]),
                ("musicgen", "facebook/musicgen-large", model_dirs["musicgen"]),
                ("audiogen", "facebook/audiogen-medium", model_dirs["audiogen"])
            ]
            
            logger.info("ðŸ”„ Checking/downloading essential models...")
            for model_name, repo_id, cache_dir in essential_models:
                download_model_from_hf(model_name, repo_id, cache_dir)

            # Try to import and initialize cinema pipeline
            logger.info("ðŸ”„ Initializing cinema pipeline...")
            
            try:
                from cinema_pipeline import CinemaPipeline, Scene
                logger.info("âœ… Cinema pipeline modules imported")
                
                # Initialize the pipeline
                pipeline = CinemaPipeline()
                logger.info("âœ… Pipeline initialized successfully!")
                logger.info("ðŸŽ¬ Ready for production video generation!")
                
                return pipeline
                
            except ImportError as e:
                logger.error(f"âŒ Cinema pipeline import failed: {e}")
                logger.error("   Creating fallback pipeline...")
                
                # Create a basic fallback pipeline
                pipeline = BasicPipeline(mode)
                logger.info("âœ… Fallback pipeline initialized")
                return pipeline
                
            except Exception as e:
                logger.error(f"âŒ Pipeline initialization failed: {e}")
                logger.error(traceback.format_exc())
                
                # Create minimal pipeline
                pipeline = MinimalPipeline()
                logger.info("âœ… Minimal pipeline initialized")
                return pipeline

        except Exception as e:
            logger.error(f"âŒ Critical initialization error: {e}")
            logger.error(traceback.format_exc())
            
            # Always return something
            pipeline = MinimalPipeline()
            logger.info("âœ… Emergency minimal pipeline initialized")
            return pipeline

class MinimalPipeline:
    """Minimal pipeline that always works"""
    
    def __init__(self):
        self.mode = "minimal"
        self.models = {}
    
    def process_script(self, script: str, options: Dict) -> Dict:
        return {
            "error": "Script processing not available in minimal mode",
            "suggestion": "Check Docker dependencies and model downloads"
        }
    
    def process_complete_scene(self, scene) -> Dict:
        return {
            "error": "Video generation not available in minimal mode", 
            "suggestion": "Check Docker dependencies and model downloads"
        }
    
    def develop_concept(self, concept: str, options: Dict) -> Dict:
        return {
            "error": "Concept development not available in minimal mode",
            "suggestion": "Check Docker dependencies and model downloads"
        }

class BasicPipeline:
    """Basic pipeline with core functionality"""
    
    def __init__(self, mode="basic"):
        self.mode = mode
        self.models = {}
        
        try:
            import torch
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Device: {self.device}")
        except:
            self.device = "cpu"
    
    def process_script(self, script: str, options: Dict) -> Dict:
        # Basic script processing
        scenes = self._parse_script_basic(script)
        return {
            "status": "success",
            "scenes": scenes,
            "mode": self.mode,
            "note": "Using basic script parsing"
        }
    
    def _parse_script_basic(self, script: str) -> List[Dict]:
        """Basic script parsing"""
        scenes = []
        blocks = script.split("\n\n")
        
        for i, block in enumerate(blocks):
            if not block.strip():
                continue
                
            lines = block.strip().split("\n")
            description = lines[0] if lines else "Scene description"
            
            scene = {
                "id": f"scene_{i+1:03d}",
                "description": description,
                "duration": 10,
                "resolution": "720p",
                "fps": 30,
                "dialogue": [],
                "camera_movements": ["static shot"],
                "music_mood": "cinematic"
            }
            
            # Extract dialogue
            for line in lines[1:]:
                if ":" in line:
                    character, text = line.split(":", 1)
                    scene["dialogue"].append({
                        "character": character.strip(),
                        "text": text.strip()
                    })
            
            scenes.append(scene)
        
        return scenes
    
    def process_complete_scene(self, scene) -> Dict:
        # Simulate processing
        time.sleep(2)
        
        return {
            "status": "success",
            "scene_id": getattr(scene, 'id', 'unknown'),
            "video_url": None,
            "audio_url": None,
            "processing_time": 2.0,
            "mode": self.mode,
            "note": "Video generation not available - models need to be loaded"
        }
    
    def develop_concept(self, concept: str, options: Dict) -> Dict:
        return {
            "status": "success",
            "concept": concept,
            "script_text": f"A story about: {concept}",
            "scenes": [],
            "mode": self.mode,
            "note": "Basic concept processing"
        }

def process_job_sync(job_input: Dict) -> Dict:
    """Synchronous job processing function"""
    request_type = job_input.get("type", "script_to_video")
    
    logger.info(f"ðŸ“‹ Processing job type: {request_type}")
    
    try:
        if request_type == "health_check":
            # Comprehensive health check
            try:
                import torch
                
                health_status = {
                    "status": "healthy" if pipeline else "unhealthy",
                    "timestamp": time.time(),
                    "system": {
                        "gpu_available": torch.cuda.is_available(),
                        "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
                        "memory_allocated": torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0,
                        "memory_cached": torch.cuda.memory_cached() / 1024**3 if torch.cuda.is_available() else 0
                    },
                    "pipeline": {
                        "initialized": pipeline is not None,
                        "mode": pipeline.mode if pipeline else "not_initialized",
                        "models_loaded": list(pipeline.models.keys()) if pipeline and hasattr(pipeline, 'models') else []
                    },
                    "volume": {
                        "id": VOLUME_ID,
                        "endpoint": S3_ENDPOINT,
                        "mounted": os.path.exists("/runpod-volume")
                    }
                }
                
                if torch.cuda.is_available():
                    health_status["gpu"] = {
                        "name": torch.cuda.get_device_name(0),
                        "vram_total_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3,
                        "vram_used_gb": torch.cuda.memory_allocated() / 1024**3,
                        "vram_free_gb": (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
                    }
                
                # Check if video models are available
                if pipeline and hasattr(pipeline, 'models'):
                    video_models = []
                    if 'ltx' in pipeline.models and pipeline.models['ltx']:
                        video_models.append("LTX-Video")
                    if 'hunyuan' in pipeline.models and pipeline.models['hunyuan']:
                        video_models.append("HunyuanVideo")
                    
                    health_status["video_models_available"] = video_models
                    
                    if not video_models and pipeline.mode != "minimal":
                        health_status["status"] = "degraded"
                        health_status["warning"] = "No video models loaded - check model downloads"
                
                return health_status
                
            except Exception as e:
                return {
                    "status": "error",
                    "timestamp": time.time(),
                    "error": str(e),
                    "pipeline_initialized": pipeline is not None
                }

        elif request_type == "single_scene":
            if not pipeline:
                return {"error": "Pipeline not initialized"}
                
            scene_data = job_input.get("scene", {})
            
            # Create simple scene object
            class SimpleScene:
                def __init__(self, data):
                    self.id = data.get("id", f"scene_{int(time.time())}")
                    self.description = data.get("description", "")
                    self.duration = data.get("duration", 5)
                    self.resolution = data.get("resolution", "720p")
                    self.fps = data.get("fps", 30)
            
            scene = SimpleScene(scene_data)
            return pipeline.process_complete_scene(scene)

        elif request_type == "script_to_video":
            if not pipeline:
                return {"error": "Pipeline not initialized"}
                
            script = job_input.get("script", "")
            if not script:
                return {"error": "No script provided"}
            
            options = job_input.get("options", {})
            return pipeline.process_script(script, options)

        elif request_type == "concept_to_script":
            if not pipeline:
                return {"error": "Pipeline not initialized"}
                
            concept = job_input.get("concept", "")
            if not concept:
                return {"error": "No concept provided"}
            
            options = job_input.get("options", {})
            return pipeline.develop_concept(concept, options)

        elif request_type == "list_models":
            return {
                "status": "success",
                "models": {
                    "video": ["HunyuanVideo-13B", "LTX-Video-13B"],
                    "audio": ["MusicGen-Large", "AudioGen-Medium"],
                    "tts": ["XTTS-v2"],
                    "active": list(pipeline.models.keys()) if pipeline and hasattr(pipeline, 'models') else []
                },
                "pipeline_mode": pipeline.mode if pipeline else "not_initialized"
            }

        else:
            return {"error": f"Unknown request type: {request_type}"}

    except Exception as e:
        logger.error(f"Job processing error: {e}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }

def handler(event):
    """RunPod serverless handler function - FIXED ASYNC PATTERN"""
    try:
        # Extract input data from the request
        input_data = event["input"]
        job_id = event.get("id", "unknown")

        logger.info(f"ðŸŽ¯ Job {job_id} started")
        logger.info(f"Type: {input_data.get('type', 'unknown')}")

        # Initialize pipeline if not done
        if pipeline is None:
            initialize_pipeline()

        # Process job synchronously (no asyncio.run)
        result = process_job_sync(input_data)

        logger.info(f"âœ… Job {job_id} completed")
        return result

    except Exception as e:
        logger.error(f"Handler error: {e}")
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }

# Initialize pipeline when module loads
try:
    # Set environment variables for optimal performance
    os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:512")
    os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "0")
    os.environ.setdefault("HF_HOME", "/runpod-volume/cache")
    os.environ.setdefault("DIFFUSERS_CACHE", "/runpod-volume/cache")
    os.environ.setdefault("AUDIOCRAFT_CACHE_DIR", "/runpod-volume/cache")
    
    # Enable HF Transfer for faster downloads
    os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")
    
    # Initialize pipeline in background
    executor.submit(initialize_pipeline)
    
except Exception as e:
    logger.error(f"Module initialization failed: {e}")

if __name__ == "__main__":
    logger.info("ðŸš€ Starting RunPod Cinema AI Worker - Production")
    logger.info("Version: 2.1 - August 2025 Edition")
    logger.info("Models: HunyuanVideo, LTX-Video, MusicGen, AudioGen, XTTS-v2")
    logger.info(f"Volume: {VOLUME_ID}")
    
    # Initialize pipeline
    initialize_pipeline()
    
    # Log configuration
    logger.info(f"Pipeline initialized: {pipeline is not None}")
    if pipeline:
        logger.info(f"Pipeline mode: {pipeline.mode}")
    
    # Start the serverless worker
    runpod.serverless.start({"handler": handler})